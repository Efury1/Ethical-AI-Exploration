
<html>
  <head>
  </head>
  <body>
  <center><h1>Model Card - IOOU AI Budget Predicter</h1></center>
  <h2>Model Details</h2>
  
<br>-- Budget Predictor AI is a machine learning model designed to predict a user’s budget (in USD) based on demographic and categorical features such as age, gender, and education level.
<br>-- The model utilizes Logistic Regression and Gaussian Naive Bayes classifiers for budget classification (above or below $300).
<br>-- The model outputs binary labels: favorable (budget >= $300) and unfavorable (budget < $300), as required for the use case.
<br>-- Fairness analysis identifies biases in predicting budgets, particularly favoring users with higher educational credentials (Bachelor's/Master’s Degrees) over high school graduates.
<br>-- Key performance metrics include accuracy, balanced accuracy, and fairness metrics such as Statistical Parity Difference and Disparate Impact.

  <h2>Intended Use</h2>
  
<br>-- The model is intended for use in the IDOOU application to enhance user experience by recommending budget-aligned activities.
<br>-- Interpretability: Results will be presented to users with explanations and options to provide feedback to ensure transparency and understanding.
<br>-- Privacy: User data (e.g., age, gender, education level) will be processed securely with encryption and anonymization to maintain privacy.
<br>-- Fairness: The app will notify users of potential model limitations and biases, prompting users to verify results and provide corrections for budget misclassification.
<br>-- The app can use fairness-aware model updates (like debiasing) in future releases to address biases identified during evaluation.

  <h2>Factors</h2>
  
<br>-- Representational Bias: The dataset shows a higher representation of privileged educational groups (Bachelor’s/Master’s Degrees), which may cause unfair model outcomes.
<br>-- Data Imbalance: Certain demographics, such as “High School Graduates,” are underrepresented, contributing to model bias.
<br>-- Model Thresholds: Threshold tuning impacts the balance between accuracy and fairness, as reflected in metrics such as balanced accuracy and equal opportunity difference.
<br>-- Fairness Trade-offs: Addressing fairness may slightly reduce accuracy but lead to better equity across groups.

  <h2>Metrics</h2>
  
<br>-- Accuracy: Measures the overall percentage of correctly predicted budgets.
<br>-- Balanced Accuracy: Averages true positive and true negative rates to account for class imbalance.
<br>-- Fairness Metrics: Includes Statistical Parity Difference (-0.98) and Disparate Impact (0.0127), indicating bias favoring privileged education groups.
<br>-- Average Odds Difference and Equal Opportunity Difference provide additional fairness assessments across protected and unprotected groups.
<br>-- Theil Index: Measures inequality in predictions to reflect deviations from expected fairness.

  <h2> Training Data </h2>
  
<br>-- The training dataset consists of ~300,000 participants' demographic and activity data, including features such as gender, age, education level, and budget.
<br>-- Missing values and duplicates were removed, and features like Age and Budget were categorized using defined bins (e.g., Age: 18-24, 25-44, Budget: <300, >=300).
<br>-- The dataset was one-hot encoded for categorical variables, with columns such as Education_Level and Budget transformed for modeling.

  <h2> Evaluation Data </h2>
  
<br>-- The evaluation dataset was split into training (50%), validation (30%), and test (20%) sets.
<br>-- Fairness analysis was performed using the AIF360 toolkit to evaluate bias against unprivileged groups (e.g., High School Graduates).
<br>-- Gaussian Naive Bayes and Logistic Regression models were evaluated using accuracy metrics and fairness metrics (Statistical Parity Difference, Disparate Impact).
<br>-- The test dataset included balanced representation for both favorable and unfavorable budget labels.

  <h2>Quantitative Analysis</h2>
  
<br>-- The Gaussian Naive Bayes model achieved a near-perfect balanced accuracy score of 1.0, but fairness metrics revealed significant bias, with a Statistical Parity Difference of -0.98 and a Disparate Impact of 0.0127, favoring privileged education groups.
<br>-- The Logistic Regression model achieved a balanced accuracy of 0.9363 after threshold tuning, with reduced but still notable bias. Statistical Parity Difference improved to -0.8694, and Disparate Impact was closer to fairness thresholds but not fully corrected.
<br>-- After applying the Reject Option Classification bias mitigation strategy, fairness metrics improved substantially. Statistical Parity Difference was reduced to approximately 0.0111, and Disparate Impact approached acceptable ranges, though some variability remained.
<br>-- Key insights showed that education level and age group were the most influential features, reflecting underlying dataset imbalances. Mitigation efforts addressed these disparities effectively while maintaining model performance for predictive accuracy.

  
  <br/><br/><b>Results of the AI model after applying the bias mitigation strategy</b><br/>
  
  <center>
  
  <img src="Lime_Feature.png" alt="LIME Feature Importance" width="600"><br/>
  <img src="Reject_Option_Classification.png" alt="Bias Mitigation Results" width="600"><br/>
  <img src="Cohort_Analysis.png" alt="Cohort Analysis Plot" width="600"><br/>

  </center>

  <h2>Ethical Considerations</h2>
  
<br>-- The dataset used in this project highlights biases stemming from an uneven representation of education levels and genders. Users with higher education credentials, such as Bachelor's or Master's degrees, are overrepresented compared to high school graduates, leading to skewed model predictions that favor privileged groups.
<br>-- The fairness analysis identified a significant disparity where the model benefited the privileged group. Statistical Parity Difference was approximately -0.98 pre-mitigation, highlighting potential unfair treatment of unprivileged groups.
<br>-- To mitigate this, the Reject Option Classification strategy was applied, which improved fairness metrics, reducing Statistical Parity Difference to near zero.
<br>-- Interpretability studies before and after bias mitigation using LIME revealed that education level, age groups, and certain activities contributed significantly to the predictions. The mitigation efforts reduced bias without severely impacting performance.

  <h2>Caveats and Recommendations</h2>
  
<br>-- The synthetic nature of the dataset limits its ability to fully reflect real-world user behaviors and preferences.
<br>-- Some demographic groups, such as individuals identifying as "Other" or "Non-binary," may still face underrepresentation, which could impact fairness and accuracy.
<br>-- Bias mitigation strategies, while effective, may slightly reduce model performance, requiring careful evaluation before deployment.
<br>-- Further ethical AI analyses I would apply beyond this project:
   - Use real-world feedback to evaluate the model's impact on diverse user groups over time.
   - Incorporate fairness-aware pre-processing techniques to improve fairness upstream in the data pipeline.

  <h2>Business Consequences</h2>
  
<br>-- Positive Impact:
   - The model enhances user satisfaction by providing personalized budget recommendations, improving user engagement and retention.
   - Hotels, autonomous vehicles, and tourism sectors benefit from improved activity recommendations, driving economic activity in local areas.
<br>-- Negative Impact:
   - Bias in the predictions could alienate underrepresented groups, damaging the brand's reputation and trust among users.
   - Failure to address fairness issues might lead to legal or ethical repercussions if users identify systematic discrimination.

  </body>
</html>